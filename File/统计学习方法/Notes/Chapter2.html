<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Chapter 2 感知机</title>
</head>
<body>
<p>
    <h3>感知机percepton是二分类线性分类模型，输入为实例的特征向量，输出为实例的类别</h3>
    <h3>感知机对应于将输入空间将实例划分为正负两类的超平面，属于判别模型</h3>
    <h3>2.1 感知机模型</h3>
    <ol>
        <li>由输入空间到输出空间的如下函数：</li>
        <li>f（x） = sign(WX + bias)称为感知机</li>
        <li>感知机的假设空间是特征空间中的所有线性分类模型</li>
    </ol>
    <h3>2.2 感知机学习策略</h3>
    <ol>
        <li>数据集要线性可分：即一定存在一个超平面，可以将数据集分为正和负两类</li>
        <li>策略：定义一个损失函数，并让损失函数最小化</li>
        <li>首先定义空间中任意一点到超平面的距离d: (Wx_0+b)/||W||</li>
        <li>对于正类y=1，Wx + b >0, 对于负类y=-1， Wx + b <0 </li>
        <li>对于所有误分类的数据 -y*(Wx + b) > 0</li>
        <li>将所有误分类数据的距离和L = -SUM(y*(Wx + b))（不考虑1/||W||）作为损失函数</li>
        <li>目标便是降低损失函数</li>
    </ol>
    <h3>2.3 感知机学习方法</h3>
    <ol>
        <li>根据上面的损失函数，我们可以求出W，b的梯度：</li>
        <ul>
            <li>用Gdt_w（）表示对w求偏导的函数</li>
            <li>Gdt_W(L) = - SUM(yx)</li>
            <li>Gdt_b(L) = - SUM(y)</li>
            <li>随机选择一个数据，对W, b进行更新：</li>
            <li>w = w + &#951;yx</li>
            <li>b = b + &#951;y</li>
            <li>上式中&#951;是学习率</li>
            <li>对上面的步骤不断进行迭代，直到损失函数L降到0.</li>
        </ul>
        <li>算法收敛性</li>
            <ul>
                <li>当训练集线性可分时，感知机的线性分类迭代是收敛的</li>
                <li>但感知机存在多重解，跟初值的选取和迭代的顺序都有关系</li>
                <li>SVM是对感知机进行约束后，求出唯一超平面的方法</li>
                <li>当训练集线性不可分时，感知机不收敛，会不断震荡</li>
            </ul>
        <li>感知机学习的对偶形式</li>
            <ul>
                <li>通过前面我们知道，通过不断的迭代，W, b最终具有下面的形式：</li>
                <li>W = SUM(&#945;yx)</li>
                <li>b = SUM(&#945;y)</li>
                <li>对偶形式的感知机是先假设出上述感知机模型，而后通过不断的迭代，通过修改&#945;，将分类正确的yx剔除，保留误分类的yx，直到没有误分类出现</li>
            </ul>
    </ol>
</p>
</body>
</html>