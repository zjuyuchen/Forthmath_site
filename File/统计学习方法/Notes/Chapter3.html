<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Chapter 3 k近邻法</title>
</head>
<body>
<p>
    <h3>k近邻法是一种基本分类与回归方法（KNN）</h3>
    <h3>输入为特征空间，输出为实例的类别，可以多类</h3>
    <h3>3.1 k近邻算法</h3>
    <p>数据集T = {（x1,y1）,(x2,y2)...(xN,yN)}</p>
    <p>yi属于集合{c1,c2...cK}，其中一共有K个分类</p>
    <ol>
        <li>根据给定的距离，在训练集中找出与x最近的K的点，涵盖这K个点的x的邻域称为N_K(x)</li>
        <li>根据分类规则：就是寻找K个点中个数最多的类别ci，将x标记为ci</li>
    </ol>
    <h3>3.2 k近邻模型</h3>
    <ol>
        <li>训练集、距离度量、K值、分类决策规则都确定后，对于任何一个新输入的实例，它所属的类是唯一确定的</li>
        <li>特征空间里面，对于一个x点，距离改点比其它点更近的所有点组成的区域叫做单元</li>
        <li>每个实例都有一个单元，所有单元构成对特征空间的划分</li>
        <li>k近邻将x的类y作为其单元所有点的类标记</li>
        <li>距离度量</li>
        <ul>
            <li>特征空间上的两个点，x1,x2，它们之间的L_p距离定义为：</li>
            <li>L_p = SUM(abs(x1- x2)^p)^(1/p)</li>
            <li>p = 1就是曼哈顿距离 L_1 = SUM(abs(x1-x2))</li>
            <li>p = 2就是欧几里得距离 L_2 = SUM((x1-x2)^2)^(1/2) 或者SQRT（SUM((x1-x2)^2)）</li>
            <li>p = inf 就是各坐标距离的最大值 L_inf = max(x1-x2)</li>
        </ul>
        <li>k值得选取</li>
        <ul>
            <li>k值小，意味着用小的邻域预测，那么近似误差会减小，但是模型就变的很复杂</li>
            <li>k值大，用大的邻域预测，整体模型变的简单，近似误差变大</li>
            <li>实际应用中，采取较小的k值，进行交叉验证来选取最优k值</li>
        </ul>
        <li>分类决策规则</li>
        <ul>
            <li>由输入实例的k个近邻训练实例中的多数类来决定输入实例的类别</li>
        </ul>
    </ol>
    <h3>3.3 k近邻法的实现，kd树</h3>
    <ol>
        <li>构造kd树， kd树是二叉树，表示对k维空间的一个划分</li>
        <ul>
            <li>构造根节点，是根节点对应包含k维空间中所有实例的超矩形区域</li>
            <li>通过下面的递归，不断地对k为空间进行切分</li>
            <li>选定一个坐标轴和切分此坐标轴的超平面，左边的超平面是一个节点，该节点代表该轴上小于节点坐标x的点的集合，右边的节点代表大于节点坐标x的点的集合，切分超平面的实例点保存在根节点中</li>
            <li>这个超平面上的根节点的在该轴的坐标应该是所有实例点在该轴坐标的中位数</li>
            <li>重复上面的操作，直到矩形区域上不存在实例点，都被划分到了根节点上</li>
        </ul>
        <li>搜索kd二叉树</li>
        <li>与搜索二叉树类似，这里不再详述</li>
        <li>kd树的计算复杂度是O（logN）,N为训练实例数目，当训练实例远大于空间维度的时候，kd树搜优效率很高</li>
        <li>当维度接近实例数时，效率迅速下降，接近线扫描</li>
    </ol>
</p>
</body>
</html>