<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Chapter 5 决策树</title>
</head>
<body>
<p>
    <h3>决策树是一种基本的分类和回归方法</h3>
    <h3>优点是可读性，分类速度快</h3>
    <h3>三个步骤：特征选择、决策树生成、决策树修建</h3>
    <h3>5.1 决策树模型与学习</h3>
    <ol>
        <li>决策树由节点和有向边组成</li>
        <li>内部节点：表示一个特征或者属性</li>
        <li>外部节点：表示一个类别</li>
        <li>决策树If-then规则</li>
        <ul>
            <li>由决策树的根节点到叶节点的每一条路径构成一条规则</li>
            <li>决策树的一个重要性质：互斥和完备</li>
            <li>每个实例都被而且只被一条路径覆盖</li>
        </ul>
        <li>决策树学习的步骤：</li>
        <ul>
            <li>构建根节点，将所有实例放在根节点，选择最优划分特征</li>
            <li>按照该特征将实例分为子集，如果子集已经能够被基本分类，则将子集分到相应的叶节点</li>
            <li>如果自己不能够基本划分，再选择最优划分特征，继续对实例进行划分，知道所有的实例都已经被分类</li>
        </ul>
        <li>以上虽然能够对实例数据进行很好的分离，但带来一个问题，就是对未知数据不一定有很好的分类能力，发生了过拟合现象，需要剪纸来避免这种情况的出现</li>
    </ol>
    <h3>5.2 特征选择</h3>
    <h3>用信息增益来判断取那个特征做划分</h3>
    <ol>
        <li>首先给出熵的定义：H（X） = -SUM(plogp)</li>
        <li>请参考: <a href="https://www.zhihu.com/question/22178202/answer/49929786">什么是信息熵？</a></li>
        <li>熵的特点是：不确定度越大，也就是p分布越平坦，则熵越大</li>
        <li>比如当随机变量只有两个值时，对应的分布符合伯努利分布，当p = 0.5时熵最大</li>
        <li>信息增益：信息熵减去条件A下D的经验条件熵之差：g(D, A) = H(D) - H(D|A)</li>
        <li>意义是：特征A的选取，造成的数据D分类不确定度的减少程度</li>
        <li>我们选择信息增益最大的特征分类</li>
        <li>用信息熵做划分数据集的特征，存在偏向选取较多特征的问题，而使用信息增益比，就可以避免这个问题</li>
    </ol>
    <h3>5.3 决策树的生成 ID3算法</h3>
    <ol>
        <li>如果D中所有类属于c，则T为单节点树，将c作为唯一的类标记</li>
        <li>如果特征集为空，则T为单节点树，并将D中最大的类c作为类标记</li>
        <li>否则，按照上面的做法选择信息增益最大的特征Ag</li>
        <li>如果Ag的信息增益小于阈值，则同2</li>
        <li>否则，按照Ag的每一个可能值，将训练集D按照Ag的种类分割为若干子集Di</li>
        <li>...</li>
        <li>由于ID3算法只进行分类，比较容易产生过拟合</li>
        <li>C4.5算法相对于ID3算法，采取的是信息增益比，而非信息增益，可以避免选择取值较多的分类</li>
    </ol>
    <h3>5.4 决策树剪枝</h3>
    <ol>
        <li>决策树中将已经生成的树进行简化的过程称为剪枝</li>
        <li>一个简单的做法是在损失函数上添加上惩罚项&#945;abs(T)，T代表模型复杂度</li>
        <li>&#945;=0的话，完全不考虑惩罚项，较大的&#945;则促使选择简单模型，较小的&#945;促使选择复杂模型</li>
    </ol>
</p>
</body>
</html>