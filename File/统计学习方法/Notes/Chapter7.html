<!DOCTYPE html>
<html lang="en">
<head>
    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
    tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]}
  });
    </script>
    <script type="text/javascript" async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <meta charset="UTF-8">
    <title>Chapter7 支持向量机</title>
</head>
<body>
    <h3>支持向量机（Support vector machines，SVM）是一种二分类模型</h3>
    <h3>定义：在特征空间中间隔最大的线性分类器</h3>
    <h3>技巧：使用核技巧可以成为实质上的非线性分类器</h3>
    <h3>学习策略：间隔最大化</h3>
    <h3>7.1 线性可分支持向量机与硬间隔最大化</h3>
    <ul>
        <li>与感知机类似，使用$wx + b$来切分超平面，不同之处是受间隔最大的约束，存在唯一解</li>
        <li>特征空间中任意一点距离超平面的距离为：</li>
        <ul>
            <li>$d = \frac{wx + b}{||w||}$,其中||w||是w的范数</li>
            <li>比如对于$Ax^{(1)} + Bx^{(2)} + C = 0, ||w|| = \sqrt{A^2 + B^2}$</li>
        </ul>
        <li>定义函数间隔:</li>
        <ul>
            <li>$\hat{\gamma}_i = y_i(wx_i+b)$</li>
            <li>其中$|wx_i+b|$能够相对的表示距离，而和$y_i$相乘可以表示分类是否一致</li>
        </ul>
        <li>对于给定训练集T和超平面$(w,b)$，超平面对于T的间距，是对所有样品点的函数间隔最小值：</li>
        <ul>
            <li>$\hat{\gamma} = \min \limits_{i = 1,...,N}\hat{\gamma}_i$</li>
        </ul>
        <li>只要我们成比例的改变$w,b$的值，我们就可以改变函数间隔的大小，但实际上，超平面并没有改变</li>
        <li>所以我们要对超平面的法向量$||w||$做约束，让其等于1，这样函数间隔就是确定的了</li>
        <li>这个就是几何间隔$\gamma_i = \frac{w}{||w||}x_i + \frac{b}{||w||}$</li>
        <li>这样，超平面$(w,b)$关于T的间距，就是对所有样品点的几何最小间隔：</li>
        <ul>
            <li>${\gamma} = \min \limits_{i = 1,...,N}{\gamma}_i$</li>
        </ul>
        <li>间隔最大化</li>
        <li>这里说的间隔是硬间隔，是对与线性可分的训练集来说的。对于线性不可分的训练集，有对应的软间隔</li>
        <li>SVM的目的是求解能够正确的划分数据集，并且几何间距最大的超平面</li>
        <li>对应下面的数学描述：</li>
        <ul>
            <li>$\max \limits_{w,b} \gamma $</li>
            <li>$s.t. y_i(\frac{w}{||w||}x_i + \frac{b}{||w||}) \geq \gamma, i = 1, 2,..., N$</li>
        </ul>
        <li>改写成函数间隔</li>
        <ul>
            <li>$\max \limits_{w,b} \frac{\hat{\gamma}}{||w||} $</li>
            <li>$s.t. y_i(wx_i+b)\geq\hat{\gamma}, i= 1, 2, ..., N$</li>
        </ul>
        <li>因为$\hat{\gamma}$的取值对超平面没有影响，我们取其为1,则问题变化为</li>
        <ul>
            <li>$\max \limits_{w,b} \frac{1}{||w||}$</li>
            <li>$s.t. y_i(wx_i+b)-1\geq 0, i= 1, 2, ..., N$</li>
            <li>$\max \limits_{w,b} \frac{1}{||w||}$等价于$\min \limits_{w,b} {||w||}^2$</li>
        </ul>
        <li>可以看出，在决定分离超平面时，只有距离超平面最近的部分点会起到效果，所以称这些重要的训练样本为支持向量</li>
        <li>支持向量学习的对偶算法</li>
        <li>对于约束问题，我们采用拉格朗日乘子法求解：</li>
        <ul>
            <li>$L(w,b,\alpha) = \frac{1}{2}{||w||}^2-\sum_{i=1}^{N}(wx_i+b) + \sum_{i=1}^N\alpha_i$</li>
            <li>根据拉格朗日对偶性，求解问题转换为极大极小问题：</li>
            <li>\max \limits_\alpha \min \limits_{w,b} L(w,b,\alpha)</li>
        </ul>
        <li>具体的求解步骤很简单，这里不再叙述，直接给出结果</li>
        <ul>
            <li>$\min \limits_\alpha \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) - \sum_{i=1}^N\alpha_i$</li>
            <li>$s.t.  \sum_{i=1}^N\alpha_iy_i = 0$</li>
            <li>$\alpha_i \geq 0, i = 1, 2, ..., N$（拉格朗日乘子满足的条件）</li>
        </ul>
        <li>则我们先求出$\alpha_i^*$， 然后再根据：</li>
        <ul>
            <li>$w^* = \sum_{i=1}^N\alpha_i^*y_ix_i$</li>
            <li>$b^* = y_j - \sum_{i=1}^N\alpha_i^*y_i(x_i\cdot x_j)$</li>
        </ul>
        <li>求出最优化问题的解</li>
    </ul>
    <h3>7.2 线性支持向量机与软间隔最大化</h3>
    <ul>
        <li>现实问题中，训练集往往是线性不可分的，这种情况下，前面提到的模型就不适用了</li>
        <li>为了解决这个问题，我们对每个样品点引入松弛变量$\xi_i\geq0$</li>
        <li>这样，约束条件就变成了$y_i(w \cdot x_i+b)\geq1-\xi_i$</li>
        <li>目标函数变为了：$\frac{1}{2}{||w||}^2 + C\sum_{i=1}^N\xi_i$, 其中$C>0$是惩罚参数</li>
        <li>优化上述函数有两个目的，前一项让间隔尽量的大，后一项让误分类的点个数尽量的小</li>
        <li>同上面，我们直接给出对偶算法的结果</li>
        <ul>
            <li>$\min \limits_\alpha \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) - \sum_{i=1}^N\alpha_i$</li>
            <li>$s.t.  \sum_{i=1}^N\alpha_iy_i = 0$</li>
            <li>$0 \leq \alpha_i \leq C, i = 1, 2, ..., N$（拉格朗日乘子满足的条件）</li>
        </ul>
        <li>上述优化问题中$w$的解是唯一的，但是$b$的解存在于一个区间，实际计算时取所有符合条件样本点的平均值</li>
    </ul>
    <h3>7.3 非线性支持向量机与核函数</h3>
    <ul>
        <li>非线性空间的映射</li>
        <ul>
            <li>假设这样一个数据集，二维平面内，满足$(x^{(1)})^2 +(x^{(2)})^2 > 1$的点是正的，剩余的是负的。</li>
            <li>对于这样的训练集合来说，线性支持向量机是无法区分的。但是如果我们对该空间进行如下映射：</li>
            <ul>
                <li>原空间到新空间的映射：$z = \phi(x) = （(x^{(1)})^2, (x^{(1)})^2 ）^T$,则在原空间区分正负样例的圆在新空间变为直线：</li>
                <li>$w_1z^{(1)} + w_2z^{(2)} = 1$</li>
            </ul>
            <li>通过上面的变换，我们把一个线性不可分的训练集，变成了在新空间线性可分的训练集，这样的方法就是核技巧</li>
        </ul>
        <li>核技巧的想法是：在学习和预测中，只定义核函数$K(x_1,x_2) = \phi(x_1)\phi(x_2)$，而不显示的定义映射函数$\phi(x)$</li>
        <li>这样做的原因是，在之前的所有计算过程中，无论是目标函数还是决策函数，我们只涉及到向量之间的内积，所以对于所有的内积$x_i \cdot x_j$</li>
        <li>我们都可以用$K(x_1,x_2) = \phi(x_1)\phi(x_2)$来代替</li>
        <li>目标函数就变为：</li>
        <ul>
            <li>$\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(K(x_1,x_2)) - \sum_{i=1}^N\alpha_i$</li>
        </ul>
        <li>常用的核函数</li>
        <li>实际问题中，往往根据经验选取已有的核函数</li>
        <li>多项式核函数</li>
        <ul>
            <li>$K(x,z) = (x \cdot z +1)^p$</li>
        </ul>
        <li>高斯核函数</li>
        <ul>
            <li>$K(x,z) = {\rm e}^{-\frac{||x-z||^2}{2\sigma^2}}$</li>
        </ul>
        <li>序列最小最优化算法</li>
        <li>当样本很多时，常规的算法会非常低效，在这种情况下，高效算法就非常重要了</li>
        <li>比较有名的有SMO算法</li>
        <ul>
            <li>先选取两个变量，固定其它变量，变成二次规划问题</li>
            <li>如果不满足KKT条件，继续分解</li>
            <li>虽然计算子问题的步骤很多。但是总体上是很高效的算法</li>
        </ul>
    </ul>
</body>
</html>